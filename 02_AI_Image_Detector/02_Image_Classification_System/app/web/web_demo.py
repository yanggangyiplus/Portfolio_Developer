"""
Streamlit ì›¹ ë°ëª¨ í˜ì´ì§€
AI ìƒì„± ì´ë¯¸ì§€ì™€ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì¸í„°ë™í‹°ë¸Œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
"""
import streamlit as st
import torch
from PIL import Image
import sys
import os
import tempfile
from pathlib import Path
from uuid import uuid4
import plotly.graph_objects as go

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
# app/web/web_demo.py -> app/web -> app -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.inference.inference import (
    load_model_for_inference,
    predict_single_image,
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI Image Detector",
    page_icon="ğŸ–¼ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ìƒìˆ˜ ì •ì˜
CLASS_NAMES = ['Real', 'AI']
CLASS_COLORS = {
    "Real": "#3498db",
    "AI": "#e74c3c"
}

# ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì •ì˜
CHECKPOINT_PATHS = {
    'cnn': Path('experiments/checkpoints/CNN_resnet18_best.pth'),
    'vit': Path('experiments/checkpoints/ViT_vit_base_best.pth')
}

# ëª¨ë¸ ì„¤ì •
MODEL_CONFIGS = {
    'cnn': {
        'model_type': 'cnn',
        'model_name': 'resnet18',
        'num_classes': 2
    },
    'vit': {
        'model_type': 'vit',
        'model_name': 'vit_base',
        'num_classes': 2
    }
}


def get_device():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒ
    
    Returns:
        str: 'cuda', 'mps', ë˜ëŠ” 'cpu'
    """
    if torch.cuda.is_available():
        return 'cuda'
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        return 'mps'
    else:
        return 'cpu'


@st.cache_resource(show_spinner=False)
def load_model(model_type='cnn', checkpoint_path=None):
    """
    ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (í†µí•©)
    
    Args:
        model_type: ëª¨ë¸ íƒ€ì… ('cnn' ë˜ëŠ” 'vit')
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
        
    Returns:
        tuple: (model, device) ë˜ëŠ” (None, None) (ë¡œë“œ ì‹¤íŒ¨ ì‹œ)
    """
    try:
        # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
        if checkpoint_path is None:
            checkpoint_path = CHECKPOINT_PATHS.get(model_type)
        
        if checkpoint_path is None or not checkpoint_path.exists():
            return None, None, f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}"
        
        # ë””ë°”ì´ìŠ¤ ì„ íƒ
        device = get_device()
        
        # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        config = MODEL_CONFIGS.get(model_type)
        if config is None:
            return None, None, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}"
        
        # ëª¨ë¸ ë¡œë“œ
        model, checkpoint = load_model_for_inference(
            checkpoint_path=checkpoint_path,
            model_type=config['model_type'],
            model_name=config['model_name'],
            num_classes=config['num_classes'],
            device=device
        )
        
        return model, device, None
        
    except FileNotFoundError as e:
        return None, None, f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}"
    except Exception as e:
        return None, None, f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"


def save_uploaded_image(uploaded_file):
    """
    ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (ì•ˆì „í•œ ë°©ì‹)
    
    Args:
        uploaded_file: Streamlit UploadedFile ê°ì²´
        
    Returns:
        str: ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        
    Raises:
        ValueError: ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ì„ ë•Œ
    """
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    temp_dir = Path('/tmp') if os.name != 'nt' else Path(tempfile.gettempdir())
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    # UUID ê¸°ë°˜ ì„ì‹œ íŒŒì¼ëª… ìƒì„±
    file_ext = Path(uploaded_file.name).suffix or '.jpg'
    temp_path = temp_dir / f"{uuid4()}{file_ext}"
    
    # ì´ë¯¸ì§€ ì €ì¥ (ì˜ˆì™¸ ì²˜ë¦¬)
    try:
        image = Image.open(uploaded_file).convert("RGB")
        image.save(temp_path, format='JPEG' if file_ext.lower() in ['.jpg', '.jpeg'] else 'PNG')
    except Exception as e:
        raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
    
    return str(temp_path)


def format_prediction_result(result):
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ë ¬ëœ êµ¬ì¡°ë¡œ ë³€í™˜í•˜ê³  íƒ€ì… ë³´ì¥
    
    Args:
        result: predict_single_imageì˜ ë°˜í™˜ê°’
        
    Returns:
        dict: ì •ë ¬ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ëª¨ë“  ìˆ«ìëŠ” Python float íƒ€ì… ë³´ì¥)
    """
    # íƒ€ì… ë³´ì¥: Tensorë‚˜ numpy íƒ€ì…ì„ Python floatë¡œ ë³€í™˜
    confidence = float(result.get("confidence", 0.0))
    probabilities = {
        k: float(v) for k, v in result.get("probabilities", {}).items()
    }
    
    return {
        "predicted_class": result["predicted_class"],
        "confidence": confidence,
        "probabilities": probabilities,
        "predicted_class_idx": int(result.get("predicted_class_idx", 0)),
        "is_ai": result.get("is_ai"),
        "image_path": result.get("image_path", "uploaded_image")
    }


def create_probability_chart(prob_data, pred_class):
    """
    í™•ë¥  ë¶„í¬ ì°¨íŠ¸ ìƒì„±
    
    Args:
        prob_data: í´ë˜ìŠ¤ë³„ í™•ë¥  ë”•ì…”ë„ˆë¦¬
        pred_class: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì´ë¦„
        
    Returns:
        plotly.graph_objects.Figure: Plotly ì°¨íŠ¸ ê°ì²´
    """
    fig = go.Figure(data=[
        go.Bar(
            x=list(prob_data.keys()),
            y=list(prob_data.values()),
            marker_color=[CLASS_COLORS.get(k, "#95a5a6") for k in prob_data.keys()],
            text=[f"{v:.2%}" for v in prob_data.values()],
            textposition='auto',
        )
    ])
    fig.update_layout(
        title="ì˜ˆì¸¡ í™•ë¥ ",
        xaxis_title="í´ë˜ìŠ¤",
        yaxis_title="í™•ë¥ ",
        yaxis=dict(range=[0, 1]),
        height=300
    )
    return fig


def handle_prediction_error(error, error_type="ì¼ë°˜"):
    """
    ì˜ˆì¸¡ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ ì²˜ë¦¬
    
    Args:
        error: Exception ê°ì²´
        error_type: ì˜¤ë¥˜ íƒ€ì… ì„¤ëª…
    """
    error_msg = str(error)
    
    if "CUDA" in error_msg or "cuda" in error_msg:
        st.error("GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ë˜ëŠ” CUDA ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    elif "format" in error_msg.lower() or "decode" in error_msg.lower():
        st.error("ì´ë¯¸ì§€ í¬ë§· ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤. (PNG, JPG, JPEGë§Œ ì§€ì›)")
    elif "memory" in error_msg.lower():
        st.error("ë©”ëª¨ë¦¬ ë¶€ì¡±: ì´ë¯¸ì§€ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ë” ì‘ì€ ì´ë¯¸ì§€ë¡œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    else:
        st.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({error_type}): {error_msg}")


def render_model_status(model, device, error_msg=None):
    """
    ëª¨ë¸ ìƒíƒœ UI ë Œë”ë§
    
    Args:
        model: ë¡œë“œëœ ëª¨ë¸ (Noneì¼ ìˆ˜ ìˆìŒ)
        device: ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤
        error_msg: ì˜¤ë¥˜ ë©”ì‹œì§€ (ìˆëŠ” ê²½ìš°)
    """
    if model is None:
        if error_msg:
            st.error(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {error_msg}")
        else:
            st.warning("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ `experiments/checkpoints/` ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # GPU ê¶Œì¥ ë©”ì‹œì§€
        if device == 'cpu':
            st.info("ğŸ’» GPUë¥¼ ì‚¬ìš©í•˜ë©´ ì¶”ë¡  ì†ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.")
    else:
        device_emoji = "ğŸš€" if device != 'cpu' else "ğŸ’»"
        st.success(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({device_emoji} {device.upper()})")


# ì œëª© ë° ì„¤ëª…
st.title("ğŸ–¼ï¸ AI Image Detector")
st.markdown("""
### ë”¥ëŸ¬ë‹ ê¸°ë°˜ AI ìƒì„± ì´ë¯¸ì§€ íƒì§€ ì‹œìŠ¤í…œ

ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ **CNN (ResNet18)** ë° **Vision Transformer (ViT)** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 
AI ìƒì„± ì´ë¯¸ì§€ì™€ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.

**ì‚¬ìš© ë°©ë²•**: ì‚¬ì´ë“œë°”ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ëª¨ë¸ì„ ì„ íƒí•œ í›„ ì˜ˆì¸¡ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.
""")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ ì„¤ì •")

# ëª¨ë¸ ì„ íƒ
model_type_radio = st.sidebar.radio(
    "ëª¨ë¸ ì„ íƒ",
    ["CNN (ResNet18)", "ViT (Vision Transformer)"],
    help="ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
)

# ëª¨ë¸ íƒ€ì… ë§¤í•‘
model_type_key = 'cnn' if model_type_radio == "CNN (ResNet18)" else 'vit'

# ëª¨ë¸ ë¡œë“œ
with st.sidebar:
    with st.spinner(f"{model_type_radio} ëª¨ë¸ ë¡œë“œ ì¤‘..."):
        model, device, error_msg = load_model(model_type=model_type_key)
        render_model_status(model, device, error_msg)

# ì´ë¯¸ì§€ ì—…ë¡œë“œ
st.sidebar.markdown("---")
uploaded_file = st.sidebar.file_uploader(
    "ğŸ“¤ ì´ë¯¸ì§€ ì—…ë¡œë“œ",
    type=['png', 'jpg', 'jpeg', 'bmp'],
    help="ë¶„ì„í•  ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”"
)

# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([1, 1])

with col1:
    st.header("ğŸ“¸ ì…ë ¥ ì´ë¯¸ì§€")
    
    if uploaded_file is not None:
        try:
            # ì´ë¯¸ì§€ ì—´ê¸° ë° RGB ë³€í™˜ (ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)
            try:
                image = Image.open(uploaded_file).convert("RGB")
            except Exception as img_error:
                st.error("ì´ë¯¸ì§€ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”.")
                st.error(f"ì˜¤ë¥˜ ìƒì„¸: {str(img_error)}")
                st.stop()
            
            st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_container_width=True)
            
            # ì´ë¯¸ì§€ ì •ë³´
            st.info(f"**ì´ë¯¸ì§€ í¬ê¸°**: {image.size[0]} Ã— {image.size[1]} pixels")
            
            # ì˜ˆì¸¡ ë²„íŠ¼
            if model is not None:
                # ì¤‘ë³µ í´ë¦­ ë°©ì§€ ì²´í¬
                if st.session_state.get("predicting", False):
                    st.warning("ì˜ˆì¸¡ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
                    st.stop()
                
                if st.button("ğŸ” ì˜ˆì¸¡í•˜ê¸°", type="primary", use_container_width=True):
                    temp_path = None
                    try:
                        # ì˜ˆì¸¡ ì‹œì‘ í”Œë˜ê·¸ ì„¤ì •
                        st.session_state["predicting"] = True
                        
                        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (ì•ˆì „í•œ ë°©ì‹)
                        temp_path = save_uploaded_image(uploaded_file)
                        
                        # ì˜ˆì¸¡ ìˆ˜í–‰
                        result = predict_single_image(
                            model=model,
                            image_path=temp_path,
                            device=device,
                            class_names=CLASS_NAMES
                        )
                        
                        # íƒ€ì… ë³´ì¥ëœ ê²°ê³¼ë¡œ ë³€í™˜
                        formatted_result = format_prediction_result(result)
                        
                        # ê²°ê³¼ë¥¼ session stateì— ì €ì¥
                        st.session_state['prediction_result'] = formatted_result
                        st.session_state['image'] = image
                        
                    except ValueError as e:
                        # ì´ë¯¸ì§€ í¬ë§· ì˜¤ë¥˜
                        handle_prediction_error(e, "ì´ë¯¸ì§€ í¬ë§· ì˜¤ë¥˜")
                    except RuntimeError as e:
                        # ë©”ëª¨ë¦¬ ë˜ëŠ” GPU ì˜¤ë¥˜
                        handle_prediction_error(e, "ëŸ°íƒ€ì„ ì˜¤ë¥˜")
                    except Exception as e:
                        # ê¸°íƒ€ ì˜¤ë¥˜
                        handle_prediction_error(e, "ì˜ˆì¸¡ ì˜¤ë¥˜")
                    finally:
                        # ì˜ˆì¸¡ ì™„ë£Œ í”Œë˜ê·¸ í•´ì œ
                        st.session_state["predicting"] = False
                        
                        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        if temp_path and os.path.exists(temp_path):
                            try:
                                os.unlink(temp_path)
                            except Exception:
                                pass  # ì‚­ì œ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            else:
                st.warning("âš ï¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                
        except Exception as e:
            st.error(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    else:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”")

with col2:
    st.header("ğŸ“Š ì˜ˆì¸¡ ê²°ê³¼")
    
    if 'prediction_result' in st.session_state:
        result = st.session_state['prediction_result']
        
        # íƒ€ì… ë³´ì¥: ì´ë¯¸ format_prediction_resultì—ì„œ ì²˜ë¦¬ë˜ì—ˆì§€ë§Œ ì•ˆì „ì„ ìœ„í•´ ì¬í™•ì¸
        pred_class = result['predicted_class']
        confidence = float(result.get('confidence', 0.0))
        prob_data = {k: float(v) for k, v in result.get('probabilities', {}).items()}
        
        # ê²°ê³¼ ì¹´ë“œ
        if pred_class == 'AI':
            st.error(f"ğŸ¤– **AI ìƒì„± ì´ë¯¸ì§€**ë¡œ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.success(f"ğŸ“· **ì‹¤ì œ ì´ë¯¸ì§€**ë¡œ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì‹ ë¢°ë„ í‘œì‹œ
        st.metric("ì‹ ë¢°ë„", f"{confidence:.2%}")
        
        # ì§„í–‰ ë°”
        st.progress(confidence)
        
        # í™•ë¥  ë¶„í¬ ì‹œê°í™”
        st.subheader("í´ë˜ìŠ¤ë³„ í™•ë¥  ë¶„í¬")
        
        fig = create_probability_chart(prob_data, pred_class)
        st.plotly_chart(fig, use_container_width=True)
        
        # ìƒì„¸ ì •ë³´
        with st.expander("ğŸ“‹ ìƒì„¸ ì •ë³´"):
            st.json(result)
        
        # í†µê³„ ì •ë³´
        st.subheader("ğŸ“ˆ í†µê³„")
        col_a, col_b = st.columns(2)
        with col_a:
            st.metric("ì˜ˆì¸¡ í´ë˜ìŠ¤", pred_class)
        with col_b:
            st.metric("í´ë˜ìŠ¤ ì¸ë±ìŠ¤", result['predicted_class_idx'])
        
    else:
        st.info("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ì˜ˆì¸¡ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

# í‘¸í„°
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>AI Image Detector | Powered by PyTorch & Streamlit</p>
    <p>CNN (ResNet18) & Vision Transformer (ViT-Base) ëª¨ë¸ ì‚¬ìš©</p>
</div>
""", unsafe_allow_html=True)
